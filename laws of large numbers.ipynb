{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THIS FIRST: FROM THE MENU AT THE TOP, CLICK CELL THEN RUN ALL.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import binom\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laws of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a sequence of coin flips, where the probability the coin lands heads is $p$. Let $X_i$ be the outcome of the $i^{th}$ flip: 1 if heads and 0 if tails. Define\n",
    "\n",
    "$$ M_n := \\frac{X_1 + \\ldots + X_n}{n} $$\n",
    "\n",
    "$M_n$ is the number of heads out of the first $n$ flips divided by $n$, or in other words the proportion of heads in the first $n$ flips.\n",
    "\n",
    "What is the value of, say, $M_3$, the proportion of heads in the first 3 flips? Well, we don't know. It might be $\\frac{0}{3}$ or $\\frac{1}{3}$ or $\\frac{2}{3}$ or $\\frac{3}{3}$, depending on whether zero, one, two or all three of the first three flips are heads. Which value it turns out to be is a chancy matter, because the outcomes of the flips are chancy. While we don't know which of these values $M_3$ is, we do know the probability that $M_3$ takes each value: the probability of heads is $p$; the probability of tails is $1-p$; and the probability of a sequence of heads and tails is the product of the probabilities of the members of the sequence, since the flips are independent. Thus: \n",
    "\n",
    "| value \t| probability \t|\n",
    "|:------:\t|:---------:\t|\n",
    "| 0     \t| $(1-p)^3$    \t|\n",
    "| 1/3   \t| $p(1-p)^2$   \t|\n",
    "| 2/3   \t| $p^2(1-p)$   \t|\n",
    "| 1     \t| $p^3$        \t|\n",
    "\n",
    "The table describes the distribution of $M_3$. In a similar way we can work out the distribution of $M_n$ for any $n$. Try graphing distributions for different values of $n$ and $p$ using the sliders below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effa5e2ee92146618e558a3e49c7dd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', min=1), FloatSlider(value=0.5, description='p', max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotBinomialDistribution(n=10, p=0.5)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bd(n,m,p):\n",
    "    return binom(n,m)*(p**m)*(1-p)**(n-m)\n",
    "\n",
    "def plotBinomialDistribution(n=10,p=0.5):\n",
    "    if p == 0 or p == 1:\n",
    "        plt.plot([p], [1], marker='o', markersize=5)\n",
    "    else:\n",
    "        plt.plot([i/n for i in range(n+1)],[bd(n,m,p) for m in range(0,n+1)],'o')\n",
    "    plt.ylabel('probability'); plt.xlabel('value of M_{}'.format(n))\n",
    "    plt.title('Distribution of M_{} when p = {}'.format(n,p))\n",
    "    plt.show()\n",
    "    \n",
    "interact(plotBinomialDistribution,n=(1,100),p=(0,1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M_n$ is within $\\epsilon$ of $p$ if the absolute value of the difference between $M_n$ and $p$ is less than $\\epsilon$. For example, if $p=\\frac{1}{2}$ then if $M_5$ equals $\\frac{2}{5}$ or $\\frac{3}{5}$ it's within $\\frac{1}{4}$ of $p$ but if $M_5$ equals 0, $\\frac{1}{5}$, $\\frac{4}{5}$ or 1 it's not. Whether $M_n$ is within $\\epsilon$ of $p$ is chancy, because the value of $M_n$ is chancy.\n",
    "\n",
    "Fix $p$ and slowly increase $n$ from 0 to 100. The graphs above suggest that as $n$ increases the probability that $M_n$ is within 0.1 of $p$ gets closer and closer to 1. In fact, the graphs suggest that as $n$ increases the probability that $M_n$ is within $\\epsilon$ of $p$ gets closer and closer to 1, for every $\\epsilon > 0$. You can graph directly the probability that $M_n$ is within $\\epsilon$ of $p$ for various values of $\\epsilon$ and $p$ using the sliders below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4137818d3a574294908986bd71e8a27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='p', max=1.0), FloatSlider(value=0.2, description='e'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotProbClose(p=0.5, e=0.2)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def probClose(n,p,e):\n",
    "    \"\"\"Computes P(|M_n - p| < e).\"\"\"\n",
    "    return np.where(np.absolute(np.array([i/n for i in range(n+1)]) - p) < e,\n",
    "             [bd(n,m,p) for m in range(0,n+1)],\n",
    "             np.zeros(n+1)).sum()\n",
    "\n",
    "def plotProbClose(p=0.5,e=0.2):\n",
    "    \"\"\"Plots P(|M_n - p| < e) against n.\"\"\"\n",
    "    plt.plot(range(1,101),[probClose(i,p,e) for i in range(1,101)])\n",
    "    plt.ylabel('probability'); plt.xlabel('n')\n",
    "    plt.title('Probability that M_n is within {} of {}.'.format(e,p))\n",
    "    plt.show()\n",
    "    \n",
    "interact(plotProbClose,p=(0,1.),e=(0.05,1,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Weak Law of Large Numbers confirms what the graphs suggest:\n",
    "\n",
    "> As $n$ tends to infinity, the probability that $M_n$, the proportion of heads in the first $n$ flips, is within $\\epsilon$ of p tends to 1, for every $\\epsilon > 0$.\n",
    "\n",
    "Here's another way to say the same thing:\n",
    "\n",
    "> $\\forall \\epsilon,\\delta > 0$ $\\exists N$ such that the probability $M_n$ is within $\\epsilon$ of $p$ is at least 1 - $\\delta$, for all $n > N$.\n",
    "\n",
    "And a third way, in fancier notation:\n",
    "\n",
    "> lim$_{n \\to \\infty}$ $P \\big($ $\\big|$ $M_n - p$ $\\big|$ < $\\epsilon \\big)$ = 1, for all $\\epsilon > 0$.\n",
    "\n",
    "For example, if you flip a fair coin often enough, then with probability at least 0.99 the proportion of heads will be within 0.01 of a half. \n",
    "\n",
    "Of course, the Weak Law doesn't just apply to coin flips. It applies to any situation with the same structure. For example, it applies to a sequence of independent gambles all with win probability $p$, telling us that as the number of gambles $n$ tends to infinity, the probability that the proportion of wins in the first $n$ gambles is within $\\epsilon$ of the win probability $p$ tends to 1, for every $\\epsilon > 0$.\n",
    "\n",
    "The Weak Law can be generalized. Instead of a sequence of coin flips, consider a sequence of rolls of a $k$-sided die. Let $Y_i$ be the result of the $i$th roll and define $M_n$ as $\\frac{Y_1 + ... + Y_n}{n}$, as before. The expectation of the die is\n",
    "\n",
    "$$ \\sum_{i=1}^{k} a_i \\cdot p_i $$\n",
    "\n",
    "where the $a_i$ are the numbers on the faces and $p_i$ is the probability of getting $a_i$. For example, for an ordinary fair die, the expectation is $\\sum_{i=1}^{6}$ $i \\cdot \\frac{1}{6}$, which is 3.5. The Weak Law tell us that:\n",
    "\n",
    "> As $n$ tends to infinity, the probability that $M_n$, the average of the first $n$ rolls, is within $\\epsilon$ of the expectation tends to 1, for every $\\epsilon > 0$.\n",
    "\n",
    "The Weak Law can be generalized further still. But the form remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the coin flips. We imagined a sequence of coin flips, $X_1$, $X_2$, $X_3$, and so on, and defined averages $M_1$, $M_2$, $M_3$, and so on. The Weak Law concerns the distribution of $M_n$ as n increases: it says, roughly speaking, that the distributions become concentrated around $p$. Now, $M_n$ depends only on the first $n$ coin flips. So to work out the distribution of $M_n$ you only have to think about how $n$ coin flips can turn out. As $n$ gets larger and larger, the number of ways $n$ coin flips can turn out gets larger and larger; but for each $n$, the number of ways is finite, and that keeps things relatively straightforward. In short: with the Weak Law, you don't need to think about how an *infinite* sequence of coin flips can turn out; you just need to think about how $n$ coin flips can turn out for each $n$. Not so for the Strong Law of Large Numbers. \n",
    "\n",
    "How can an infinite sequence of coin flips turn out? Well, it might be that every flip is tails, which we represent with an infinite sequence of 0's: (0,0,0,0,...). Or it might be that the first flip is heads and the rest are tails, which we represent with the sequence (1,0,0,0,...). Or it might be that the even-numbered flips are heads and the odd-numbered flips are tails, which we represent with the sequence (0,1,0,1,...). And so on. There are infinitely many---in fact, *uncountably many*---ways an infinite sequence of coin flips can turn out. We'll represent them with the set of all infinite sequences of 0's and 1's. Where $s$ is a sequence, $s_n$ is its $n^{th}$ entry and $s[k]$ is the finite sequence consisting of its first $k$ entries.\n",
    "\n",
    "The event that the first flip is heads is the set of all sequences $s$ such that $s_1 = 1$. The event that the third flip is tails and the seventeenth is heads is the set of all sequences $s$ such that $s_3 = 0$ and $s_{17} = 1$. The event that there are at least three heads in the first 10 flips is the set of all sequences $s$ such that $s[10]$ contains at least three 1's. The event that the number of tails is always ahead of the number of heads is the set of all sequences $s$ such that: there are more 0's than 1's in $s[n]$ for every $n$. The event that the proportion of heads in the first $n$ flips tends to 1/2 as $n$ tends to infinity is the set of all sequences $s$ such that: the number of 1's in $s[n]$ divided by $n$ tends to 1/2 as $n$ tends to infinity.\n",
    "\n",
    "To define a probability measure on infinite sequences of coin flips is to find a sensible way to assign probabilities to events like those in the previous paragraph. What does *sensible* mean? Well, for example: the probability assigned to the event that the first flip is heads should be $p$; the probability assigned to the event that the third flip is tails and the seventeenth is heads should be $(1-p)p$; the probability assigned to the event that there are at least three heads in the first 10 flips should be $1 - {10 \\choose 2} \\cdot p^2(1-p)^8 - {10 \\choose 1} \\cdot p(1-p)^9 - (1-p)^{10}$. \n",
    "\n",
    "It turns out, happily, that it is possible to define a probability measure on infinite sequences of coin flips. Given that, we can at last state the Strong Law of Large Numbers:\n",
    "\n",
    "> The probability assigned to the event that the proportion of heads in the first $n$ flips tends to $p$ as $n$ tends to infinity is 1. \n",
    "\n",
    "We can write (lim$_{n \\to \\infty} M_n) = p$ for the event that the proportion of heads in the first $n$ flips tends to $p$ as $n$ tends to infinity. And where $E$ is an event, $P(E)$ is the probability assigned to $E$. So another way to state the Strong Law is:\n",
    "\n",
    "> P $\\big($ (lim$_{n \\to \\infty} M_n) = p \\big )$ = 1.\n",
    "\n",
    "Like the Weak Law, the Stong Law doesn't just apply to coin flips. It applies to any situation with the same structure. For example, it applies to a sequence of mutually independent gambles all with win probability $p$, telling us that the probability that the proportion of wins in the first $n$ gambles tends to $p$ as $n$ tends to infinity is 1.\n",
    "\n",
    "Like the Weak Law, the Strong Law can be generalized. For example:\n",
    "\n",
    "> The probability assigned to the event that the average of $n$ rolls of a die tends to the expectation of the die as $n$ tends to infinity is 1.\n",
    "\n",
    "It can be generalized further still. But the form remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Weak and Strong Laws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Weak Law involves a potential infinity; the Strong Law involves a completed infinity. The Weak Law says that the limit of a sequence of probabilities is 1; the Strong Law says that a particular probability is 1. In the Weak Law, we take a limit outside a probability; in the Strong Law, we take a limit inside a probability.\n",
    "\n",
    "Terry Tao suggests in a [blog post](https://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/) how to keep the Weak and Strong Laws straight. Consider the table:\n",
    "\n",
    "\n",
    "| $\\phantom{x}$            | $M_1$ | $M_2$      | $M_3$         \t| $M_4$         \t| $M_5$             | ... \t\n",
    "|-----------------\t| ---- | ---------------\t| ---------------\t| ---------------\t| ---------------\t| ----\t\n",
    "| (0,0,0,0,0,...) \t| 0    | 0            \t    | 0                 | 0            \t    | 0            \t\n",
    "| (0,0,1,0,1,...) \t| 0    | 0                  | $\\frac{1}{3}$     | $\\frac{1}{4}$     | $\\frac{2}{5}$\n",
    "| (1,0,1,1,0,...) \t| 1    | $\\frac{1}{2}$      | $\\frac{2}{3}$     | $\\frac{3}{4}$     | $\\frac{3}{5}$\t     \n",
    "| (1,1,1,0,0,...) \t| 1    | 1                  | 1                 | $\\frac{3}{4}$     | $\\frac{3}{5}$ \n",
    "| ... \n",
    "\n",
    "The rows of the table are indexed by infinite sequences of coin flips. (So there should be a continuum of rows, but let's not worry about that.) The $n^{th}$ column records $M_n$, the proportion of heads in the first $n$ flips. Fix $p$ and $\\epsilon > 0$, and circle the entries which are *not* within $\\epsilon$ of $p$. (Try it in the table above for $p$ = 1/2 and $\\epsilon$ = 1/5, say.) The Weak Law is about columns: it says that as you move to the right column by column, the probability of the set of sequences yielding a circled entry tends to 0; or in other words, as you move to the right column by column, the density of circled entries tends to 0. The Strong Law is about rows: it says that the probability assigned to the set of rows with finitely many circled entries is 1; or in other words, almost all the rows have finitely many circled entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of $M_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_1$, $x_2$, $x_3$, ... be a sequence of real numbers. Here's a useful fact:\n",
    "\n",
    "> If $x_n \\to c$ then $f(x_n) \\to f(c)$, for any continuous function $f : \\mathbb{R} \\to \\mathbb{R}$.\n",
    "\n",
    "For example, $\\frac{1}{n} \\to 0$ and $f(x) = e^x$ is continuous, so $e^{\\frac{1}{n}} \\to e^0 = 1$.\n",
    "\n",
    "Now, two definitions. Let $X_1$, $X_2$, $X_3$, ... be a sequence of random variables and $c$ a number.\n",
    "\n",
    "> We say that $X_n$ tends to $c$ *in probability*, or $X_n \\to_{i.p.} c$ for short, if lim$_{n \\to \\infty}$ $P$ $\\big($ $\\big|$ $X_n - c$ $\\big|$ < $\\epsilon$ $\\big)$ = 1, for all $\\epsilon > 0$.\n",
    "\n",
    "> We say that $X_n$ tends to $c$ *almost surely*, or $X_n \\to_{a.s.} c$ for short, if $P \\big($ (lim$_{n \\to \\infty} X_n) = c \\big )$ = 1.\n",
    "\n",
    "So yet another way to state the Weak Law is that $M_n \\to_{i.p.} p$ and yet another way to state the Strong Law is that $M_n \\to_{a.s.} p$.\n",
    "\n",
    "Here's the punchline. As with sequences of real numbers, so with sequences of random variables:\n",
    "\n",
    "> If $X_n \\to_{i.p.} c$ then $f(X_n) \\to_{i.p.} f(c)$, and if $X_n \\to_{a.s.} c$ then $f(X_n) \\to_{a.s.} f(c)$, for any continuous function $f : \\mathbb{R} \\to \\mathbb{R}$.\n",
    "\n",
    "For example, $M_n$ tends in probability and almost surely to $p$, so $e^{M_n}$ tends in probability and almost surely to $e^p$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
